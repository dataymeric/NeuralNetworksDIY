<!doctype html>
<html class="no-js" lang="fr">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Recherche" href="search.html" />

    <!-- Generated with Sphinx 6.2.1 and Furo 2023.03.27 -->
        <title>src package - Documentation Neural Networks DIY</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">Documentation Neural Networks DIY </div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  
  <span class="sidebar-brand-text">Documentation Neural Networks DIY </span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Recherche" name="q" aria-label="Recherche">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  
</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="src-package">
<h1>src package<a class="headerlink" href="#src-package" title="Lien permanent vers cette rubrique">#</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Lien permanent vers cette rubrique">#</a></h2>
</section>
<section id="module-src.activation">
<span id="src-activation-module"></span><h2>src.activation module<a class="headerlink" href="#module-src.activation" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.activation.LeakyReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">LeakyReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Leaky ReLU activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\text{LeakyReLU}(x) = \max(\alpha x, x) =
\begin{cases}
x, &amp; \text{ if } x \geq 0 \\
\alpha \times x, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial M}{\partial z^h} = \begin{cases}
    1 &amp; \text{if } x&gt;0, \\
    \alpha &amp; \text{otherwise}.
\end{cases}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.LogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">LogSoftmax</span></span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>LogSoftmax activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{LogSoftmax}(x_{i}) =
\log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)} \right)\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><a class="reference internal" href="_modules/src/activation.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>ReLU (rectified linear unit) activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{ReLU}(x) = x^+ = \max(0, x)\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = 1 \text{ if } x &gt; 0 \text{ else } 0.\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><a class="reference internal" href="_modules/src/activation.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sigmoid activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = \sigma(z^h) * (1 - \sigma(z^h))\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.Softmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><a class="reference internal" href="_modules/src/activation.html#Softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Softmax activation function.
Commonly used along with a cross entropy loss. See [Softmax and cross-entropy loss](<a class="reference external" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a>) and [Derivative of Cross Entropy Loss with Softmax](<a class="reference external" href="https://www.parasdahal.com/softmax-crossentropy">https://www.parasdahal.com/softmax-crossentropy</a>)</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M(x_i)}{\partial x_i} = M^h(x_i) * (1 - M^h(x_i))\]</div>
</div>
<p>Plus prÃ©cisement</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial M^h(x_i)}{x_j} = \begin{cases}
    M^h(x_i) * ( 1 - M^h(x_j) ) &amp;\text{si } i = j \\
    - M^h(x_j) M^h(x_i) &amp;\text{ si } i \neq j \\
\end{cases}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Implemented using a log sum exp trick to avoid NaN. See [Computing softmax and numerical stability](<a class="reference external" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a>).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.Softplus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">Softplus</span></span><a class="reference internal" href="_modules/src/activation.html#Softplus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Smooth approximation of the ReLU activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{Softplus}(x) = \ln(1 + e^x)\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = \sigma (x) = \frac{1}{1 + e^{-x}}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.StableSigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">StableSigmoid</span></span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Numerically stable Sigmoid activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.TanH">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">TanH</span></span><a class="reference internal" href="_modules/src/activation.html#TanH"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Hyperbolic Tangent activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \text{TanH}(x) &amp;= \tanh(x)  \\
        &amp;= \frac{\sinh x}{\cosh x} \\
        &amp;= \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)} \\
        &amp;= \frac{e^{2x} - 1 }{e^{2x} + 1}
    \end{align*}\end{split}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = 1 - \tanh (z^h)^2\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
<section id="src-convolution-module">
<h2>src.convolution module<a class="headerlink" href="#src-convolution-module" title="Lien permanent vers cette rubrique">#</a></h2>
<p>We tried to vectorize our convolutions to the maximum, prioritizing the performance.</p>
<p>It implies creating special views of our array, by using the <cite>numpy.lib.stride_tricks</cite>
functions. <cite>sliding_window_view</cite> is the easiest to understand, while maybe not the
fastest compared to <cite>as_strided</cite> (but maybe less risky too).</p>
<p>The calculations are done using <cite>np.einsum</cite>, which is relatively easy to understand
and use. The key relies in understanding the shapes of your inputs/outputs.</p>
</section>
<section id="shape">
<h2>Shape<a class="headerlink" href="#shape" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="simple">
<dt>Reminder for 1D:</dt><dd><ul class="simple">
<li><p>input : ndarray (batch, length, chan_in)</p></li>
<li><p>d_out : ndarray (batch, length, chan_in) == input.shape</p></li>
<li><p>X_view : ndarray (batch, out_length, chan_in, self.k_size)</p></li>
<li><p>delta : ndarray (batch, out_length, chan_out)</p></li>
<li><p>_gradient[Â«Â weightÂ Â»] : ndarray (k_size, chan_in, chan_out)</p></li>
<li><p>_parameters[Â«Â weightÂ Â»] : ndarray (k_size, chan_in, chan_out)</p></li>
</ul>
</dd>
</dl>
</section>
<section id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="simple">
<dt>Notation used for <cite>np.einsum</cite>:</dt><dd><ul class="simple">
<li><p>b : batch_size</p></li>
<li><p>w : width (2D) / length (1D)</p></li>
<li><p>h : height (2D)</p></li>
<li><p>o : out_width (2D) / out_length (1D)</p></li>
<li><p>p : out_height (2D)</p></li>
<li><p>c : chan_in</p></li>
<li><p>d : chan_out</p></li>
<li><p>k : k_size (ij for 2D)</p></li>
</ul>
</dd>
</dl>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Lien permanent vers cette rubrique">#</a></h2>
<p>Quick demonstration of <cite>sliding_window_view</cite> in 1D:
.. code-block:: python</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">batch</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">chan_in</span><span class="p">,</span> <span class="n">k_size</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">chan_in</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">input</span>
<span class="go">array([[[-0.41982262],</span>
<span class="go">      [ 1.10111123],</span>
<span class="go">      [-0.41115195],</span>
<span class="go">      [ 1.18733225],</span>
<span class="go">      [-1.93463567],</span>
<span class="go">      [-0.22472025],</span>
<span class="go">      [-0.30581971],</span>
<span class="go">      [ 0.40578667]]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">stride_tricks</span><span class="o">.</span><span class="n">sliding_window_view</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k_size</span><span class="p">,</span> <span class="n">chan_in</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span>
<span class="go">array([[[[[[-0.41982262],</span>
<span class="go">         [ 1.10111123],</span>
<span class="go">         [-0.41115195]]]],</span>
<span class="go">      [[[[ 1.10111123],</span>
<span class="go">         [-0.41115195],</span>
<span class="go">         [ 1.18733225]]]],</span>
<span class="go">   ...</span>
</pre></div>
</div>
<p>How to deal with stride != 1?
.. code-block:: python</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stride</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">lib</span><span class="o">.</span><span class="n">stride_tricks</span><span class="o">.</span><span class="n">sliding_window_view</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">k_size</span><span class="p">,</span> <span class="n">chan_in</span><span class="p">))[::</span><span class="mi">1</span><span class="p">,</span> <span class="p">::</span><span class="n">stride</span><span class="p">,</span> <span class="p">::</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span>
<span class="go">array([[[[[[-0.41982262],</span>
<span class="go">         [ 1.10111123],</span>
<span class="go">         [-0.41115195]]]],</span>
<span class="go">      [[[[ 1.18733225],</span>
<span class="go">         [-1.93463567],</span>
<span class="go">         [-0.22472025]]]]]])</span>
</pre></div>
</div>
<p>Then it is just a matter of reshape, to drop unnecessaries dimensions, e.g. :
.. code-block:: python</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">window</span> <span class="o">=</span> <span class="n">window</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">out_length</span><span class="p">,</span> <span class="n">chan_in</span><span class="p">,</span> <span class="n">k_size</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">window</span>
<span class="go">array([[[[-0.41982262,  1.10111123, -0.41115195]],</span>
<span class="go">      [[ 1.18733225, -1.93463567, -0.22472025]]]])</span>
</pre></div>
</div>
<p>And voilÃ !</p>
<span class="target" id="module-src.convolution"></span><dl class="py class">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">AvgPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D average pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) â Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) â Stride of the convolution.</p></li>
</ul>
</dd>
</dl>
<section id="id1">
<h3>Shape<a class="headerlink" href="#id1" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, (length - k_size) // stride + 1, chan_out)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.convolution.Conv1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">Conv1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'zeros'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'ones'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_uniform'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'xavier_normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) â Size of the convolving kernel.</p></li>
<li><p><strong>chan_in</strong> (<em>int</em>) â Number of channels in the input image.</p></li>
<li><p><strong>chan_out</strong> (<em>in</em>) â Number of channels produced by the convolution.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) â Stride of the convolution.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) â If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;xavier_normal&quot;</em>) â Change the initialization of parameters.</p></li>
</ul>
</dd>
</dl>
<section id="id2">
<h3>Shape<a class="headerlink" href="#id2" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, (length - k_size) // stride + 1, chan_out)</p></li>
<li><p>Weight : ndarray (k_size, chan_in, chan_out)</p></li>
<li><p>Bias : ndarray (chan_out)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.convolution.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><a class="reference internal" href="_modules/src/convolution.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Flatten an output.</p>
<section id="id3">
<h3>Shape<a class="headerlink" href="#id3" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, length * chan_in)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">MaxPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D max pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) â Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) â Stride of the convolution.</p></li>
</ul>
</dd>
</dl>
<section id="id4">
<h3>Shape<a class="headerlink" href="#id4" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, (length - k_size) // stride + 1, chan_out)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-src.encapsulation">
<span id="src-encapsulation-module"></span><h2>src.encapsulation module<a class="headerlink" href="#module-src.encapsulation" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.encapsulation.Optim">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.encapsulation.</span></span><span class="sig-name descname"><span class="pre">Optim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><span class="pre">Loss</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.SGD">
<span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">42</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.SGD" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.SGD_eval">
<span class="sig-name descname"><span class="pre">SGD_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_train</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_test</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">42</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dataframe</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">online_plot</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.SGD_eval" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.score" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.step" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.encapsulation.Sequential">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.encapsulation.</span></span><span class="sig-name descname"><span class="pre">Sequential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.add"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.add" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Add a module to the network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.insert">
<span class="sig-name descname"><span class="pre">insert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.insert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.insert" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Insert a module to the network at a specified indice.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.reset" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Reset network to initial parameters and modules.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-src.linear">
<span id="src-linear-module"></span><h2>src.linear module<a class="headerlink" href="#module-src.linear" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.linear.Linear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.linear.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'zeros'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'ones'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_uniform'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'he_normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Linear module.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) â Size of input sample.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) â Size of output sample.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) â If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;normal&quot;</em>) â Change the initialization of parameters.</p></li>
</ul>
</dd>
</dl>
<section id="id5">
<h3>Shape<a class="headerlink" href="#id5" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, input_size)</p></li>
<li><p>Output : ndarray (batch, output_size)</p></li>
<li><p>Weight : ndarray (input_size, output_size)</p></li>
<li><p>Bias : ndarray (1, output_size)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Forward pass.</p>
<p class="rubric">Notes</p>
<p>X &#64; w = (batch, input_size) &#64; (input_size, output_size) = (batch, output_size)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

</section>
<section id="module-src.loss">
<span id="src-loss-module"></span><h2>src.loss module<a class="headerlink" href="#module-src.loss" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.loss.BCELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">BCELoss</span></span><a class="reference internal" href="_modules/src/loss.html#BCELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.BCELoss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Binary Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.BCELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.BCELoss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.BCELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.BCELoss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.loss.CELogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">CELogSoftmax</span></span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CELogSoftmax" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{CE}(y, \hat{y}) = - \log \frac {e^{\hat{y}_y}} {\sum_{i=1}^{K}
e^{\hat{y}_i}} = -\hat{y}_y} + \log \sum_{i=1}^{K}e^{\hat{y}_i}}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CELogSoftmax.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CELogSoftmax.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CELogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CELogSoftmax.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.loss.CrossEntropyLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLoss</span></span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CrossEntropyLoss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CrossEntropyLoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CrossEntropyLoss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CrossEntropyLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CrossEntropyLoss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.loss.MSELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">MSELoss</span></span><a class="reference internal" href="_modules/src/loss.html#MSELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.MSELoss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Mean Squared Error loss function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[MSE = ||y - \hat{y}||^2\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\nabla_{MSE} = -2(y - \hat{y})\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.MSELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.MSELoss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.MSELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.MSELoss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-src.module">
<span id="src-module-module"></span><h2>src.module module<a class="headerlink" href="#module-src.module" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.module.Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.module.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="reference internal" href="_modules/src/module.html#Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Loss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.module.Loss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Loss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Loss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.module.Module">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.module.</span></span><span class="sig-name descname"><span class="pre">Module</span></span><a class="reference internal" href="_modules/src/module.html#Module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.calculate_gain">
<span class="sig-name descname"><span class="pre">calculate_gain</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.calculate_gain"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.calculate_gain" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-src">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-src" title="Lien permanent vers cette rubrique">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.AvgPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">AvgPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D average pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) â Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) â Stride of the convolution.</p></li>
</ul>
</dd>
</dl>
<section id="id6">
<h3>Shape<a class="headerlink" href="#id6" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, (length - k_size) // stride + 1, chan_out)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.BCELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">BCELoss</span></span><a class="reference internal" href="_modules/src/loss.html#BCELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.BCELoss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Binary Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.BCELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.BCELoss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.BCELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.BCELoss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.CELogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">CELogSoftmax</span></span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CELogSoftmax" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{CE}(y, \hat{y}) = - \log \frac {e^{\hat{y}_y}} {\sum_{i=1}^{K}
e^{\hat{y}_i}} = -\hat{y}_y} + \log \sum_{i=1}^{K}e^{\hat{y}_i}}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.CELogSoftmax.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CELogSoftmax.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.CELogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CELogSoftmax.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Conv1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Conv1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'zeros'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'ones'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_uniform'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'xavier_normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) â Size of the convolving kernel.</p></li>
<li><p><strong>chan_in</strong> (<em>int</em>) â Number of channels in the input image.</p></li>
<li><p><strong>chan_out</strong> (<em>in</em>) â Number of channels produced by the convolution.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) â Stride of the convolution.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) â If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;xavier_normal&quot;</em>) â Change the initialization of parameters.</p></li>
</ul>
</dd>
</dl>
<section id="id7">
<h3>Shape<a class="headerlink" href="#id7" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, (length - k_size) // stride + 1, chan_out)</p></li>
<li><p>Weight : ndarray (k_size, chan_in, chan_out)</p></li>
<li><p>Bias : ndarray (chan_out)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.CrossEntropyLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLoss</span></span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CrossEntropyLoss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.CrossEntropyLoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CrossEntropyLoss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.CrossEntropyLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CrossEntropyLoss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><a class="reference internal" href="_modules/src/convolution.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Flatten an output.</p>
<section id="id8">
<h3>Shape<a class="headerlink" href="#id8" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, length * chan_in)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Linear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'zeros'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'ones'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'he_uniform'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_normal'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'xavier_uniform'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'he_normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Linear module.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) â Size of input sample.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) â Size of output sample.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) â If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;normal&quot;</em>) â Change the initialization of parameters.</p></li>
</ul>
</dd>
</dl>
<section id="id9">
<h3>Shape<a class="headerlink" href="#id9" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, input_size)</p></li>
<li><p>Output : ndarray (batch, output_size)</p></li>
<li><p>Weight : ndarray (input_size, output_size)</p></li>
<li><p>Bias : ndarray (1, output_size)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Forward pass.</p>
<p class="rubric">Notes</p>
<p>X &#64; w = (batch, input_size) &#64; (input_size, output_size) = (batch, output_size)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.LogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">LogSoftmax</span></span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>LogSoftmax activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{LogSoftmax}(x_{i}) =
\log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)} \right)\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="reference internal" href="_modules/src/module.html#Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Loss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Loss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Loss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Loss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.MSELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">MSELoss</span></span><a class="reference internal" href="_modules/src/loss.html#MSELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MSELoss" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Mean Squared Error loss function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[MSE = ||y - \hat{y}||^2\]</div>
</div>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\nabla_{MSE} = -2(y - \hat{y})\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.MSELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MSELoss.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MSELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MSELoss.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.MaxPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">MaxPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D max pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">ParamÃ¨tres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) â Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) â Stride of the convolution.</p></li>
</ul>
</dd>
</dl>
<section id="id10">
<h3>Shape<a class="headerlink" href="#id10" title="Lien permanent vers cette rubrique">#</a></h3>
<ul class="simple">
<li><p>Input : ndarray (batch, length, chan_in)</p></li>
<li><p>Output : ndarray (batch, (length - k_size) // stride + 1, chan_out)</p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Module">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Module</span></span><a class="reference internal" href="_modules/src/module.html#Module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Module.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.calculate_gain">
<span class="sig-name descname"><span class="pre">calculate_gain</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.calculate_gain"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.calculate_gain" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Optim">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Optim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><span class="pre">Loss</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.SGD">
<span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">42</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.SGD" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.SGD_eval">
<span class="sig-name descname"><span class="pre">SGD_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_train</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_test</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">42</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dataframe</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">online_plot</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.SGD_eval" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.score" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.step" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><a class="reference internal" href="_modules/src/activation.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>ReLU (rectified linear unit) activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{ReLU}(x) = x^+ = \max(0, x)\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = 1 \text{ if } x &gt; 0 \text{ else } 0.\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Sequential">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Sequential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.add"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.add" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Add a module to the network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.backward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.insert">
<span class="sig-name descname"><span class="pre">insert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.insert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.insert" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Insert a module to the network at a specified indice.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.reset" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Reset network to initial parameters and modules.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><a class="reference internal" href="_modules/src/activation.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sigmoid activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = \sigma(z^h) * (1 - \sigma(z^h))\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Softmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><a class="reference internal" href="_modules/src/activation.html#Softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Softmax activation function.
Commonly used along with a cross entropy loss. See [Softmax and cross-entropy loss](<a class="reference external" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a>) and [Derivative of Cross Entropy Loss with Softmax](<a class="reference external" href="https://www.parasdahal.com/softmax-crossentropy">https://www.parasdahal.com/softmax-crossentropy</a>)</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M(x_i)}{\partial x_i} = M^h(x_i) * (1 - M^h(x_i))\]</div>
</div>
<p>Plus prÃ©cisement</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial M^h(x_i)}{x_j} = \begin{cases}
    M^h(x_i) * ( 1 - M^h(x_j) ) &amp;\text{si } i = j \\
    - M^h(x_j) M^h(x_i) &amp;\text{ si } i \neq j \\
\end{cases}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Implemented using a log sum exp trick to avoid NaN. See [Computing softmax and numerical stability](<a class="reference external" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/">https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/</a>).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Softplus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Softplus</span></span><a class="reference internal" href="_modules/src/activation.html#Softplus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Smooth approximation of the ReLU activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{Softplus}(x) = \ln(1 + e^x)\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = \sigma (x) = \frac{1}{1 + e^{-x}}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.TanH">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">TanH</span></span><a class="reference internal" href="_modules/src/activation.html#TanH"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Hyperbolic Tangent activation function.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \text{TanH}(x) &amp;= \tanh(x)  \\
        &amp;= \frac{\sinh x}{\cosh x} \\
        &amp;= \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)} \\
        &amp;= \frac{e^{2x} - 1 }{e^{2x} + 1}
    \end{align*}\end{split}\]</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.backward_delta" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\frac{\partial M}{\partial z^h} = 1 - \tanh (z^h)^2\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.backward_update_gradient" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.forward" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.update_parameters" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.zero_grad" title="Lien permanent vers cette dÃ©finition">#</a></dt>
<dd><p>RÃ©initialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023, Charles Vin &amp; Aymeric Delefosse
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">src package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-src.activation">src.activation module</a><ul>
<li><a class="reference internal" href="#src.activation.LeakyReLU"><code class="docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.LeakyReLU.backward_delta"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.forward"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.update_parameters"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.zero_grad"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.LogSoftmax"><code class="docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.LogSoftmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.ReLU"><code class="docutils literal notranslate"><span class="pre">ReLU</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.ReLU.backward_delta"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.forward"><code class="docutils literal notranslate"><span class="pre">ReLU.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.update_parameters"><code class="docutils literal notranslate"><span class="pre">ReLU.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.zero_grad"><code class="docutils literal notranslate"><span class="pre">ReLU.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.Sigmoid"><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.Sigmoid.backward_delta"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.forward"><code class="docutils literal notranslate"><span class="pre">Sigmoid.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sigmoid.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sigmoid.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.Softmax"><code class="docutils literal notranslate"><span class="pre">Softmax</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.Softmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.forward"><code class="docutils literal notranslate"><span class="pre">Softmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.Softplus"><code class="docutils literal notranslate"><span class="pre">Softplus</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.Softplus.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.forward"><code class="docutils literal notranslate"><span class="pre">Softplus.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softplus.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softplus.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.StableSigmoid"><code class="docutils literal notranslate"><span class="pre">StableSigmoid</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.StableSigmoid.backward_delta"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.forward"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.update_parameters"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.zero_grad"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.TanH"><code class="docutils literal notranslate"><span class="pre">TanH</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.TanH.backward_delta"><code class="docutils literal notranslate"><span class="pre">TanH.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">TanH.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.forward"><code class="docutils literal notranslate"><span class="pre">TanH.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.update_parameters"><code class="docutils literal notranslate"><span class="pre">TanH.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.zero_grad"><code class="docutils literal notranslate"><span class="pre">TanH.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#src-convolution-module">src.convolution module</a></li>
<li><a class="reference internal" href="#shape">Shape</a></li>
<li><a class="reference internal" href="#notes">Notes</a></li>
<li><a class="reference internal" href="#examples">Examples</a><ul>
<li><a class="reference internal" href="#src.convolution.AvgPool1D"><code class="docutils literal notranslate"><span class="pre">AvgPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.forward"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.convolution.Conv1D"><code class="docutils literal notranslate"><span class="pre">Conv1D</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.Conv1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.forward"><code class="docutils literal notranslate"><span class="pre">Conv1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">Conv1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">Conv1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.convolution.Flatten"><code class="docutils literal notranslate"><span class="pre">Flatten</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.Flatten.backward_delta"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.forward"><code class="docutils literal notranslate"><span class="pre">Flatten.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.update_parameters"><code class="docutils literal notranslate"><span class="pre">Flatten.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.zero_grad"><code class="docutils literal notranslate"><span class="pre">Flatten.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D"><code class="docutils literal notranslate"><span class="pre">MaxPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.forward"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.encapsulation">src.encapsulation module</a><ul>
<li><a class="reference internal" href="#src.encapsulation.Optim"><code class="docutils literal notranslate"><span class="pre">Optim</span></code></a><ul>
<li><a class="reference internal" href="#src.encapsulation.Optim.SGD"><code class="docutils literal notranslate"><span class="pre">Optim.SGD()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Optim.SGD_eval"><code class="docutils literal notranslate"><span class="pre">Optim.SGD_eval()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Optim.score"><code class="docutils literal notranslate"><span class="pre">Optim.score()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Optim.step"><code class="docutils literal notranslate"><span class="pre">Optim.step()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.encapsulation.Sequential"><code class="docutils literal notranslate"><span class="pre">Sequential</span></code></a><ul>
<li><a class="reference internal" href="#src.encapsulation.Sequential.add"><code class="docutils literal notranslate"><span class="pre">Sequential.add()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.backward"><code class="docutils literal notranslate"><span class="pre">Sequential.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.forward"><code class="docutils literal notranslate"><span class="pre">Sequential.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.insert"><code class="docutils literal notranslate"><span class="pre">Sequential.insert()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.reset"><code class="docutils literal notranslate"><span class="pre">Sequential.reset()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sequential.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sequential.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.linear">src.linear module</a><ul>
<li><a class="reference internal" href="#src.linear.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a><ul>
<li><a class="reference internal" href="#src.linear.Linear.backward_delta"><code class="docutils literal notranslate"><span class="pre">Linear.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Linear.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.forward"><code class="docutils literal notranslate"><span class="pre">Linear.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.update_parameters"><code class="docutils literal notranslate"><span class="pre">Linear.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.zero_grad"><code class="docutils literal notranslate"><span class="pre">Linear.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.loss">src.loss module</a><ul>
<li><a class="reference internal" href="#src.loss.BCELoss"><code class="docutils literal notranslate"><span class="pre">BCELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.BCELoss.backward"><code class="docutils literal notranslate"><span class="pre">BCELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.BCELoss.forward"><code class="docutils literal notranslate"><span class="pre">BCELoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.loss.CELogSoftmax"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.CELogSoftmax.backward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.CELogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.loss.CrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.CrossEntropyLoss.backward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.CrossEntropyLoss.forward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.loss.MSELoss"><code class="docutils literal notranslate"><span class="pre">MSELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.MSELoss.backward"><code class="docutils literal notranslate"><span class="pre">MSELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.MSELoss.forward"><code class="docutils literal notranslate"><span class="pre">MSELoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.module">src.module module</a><ul>
<li><a class="reference internal" href="#src.module.Loss"><code class="docutils literal notranslate"><span class="pre">Loss</span></code></a><ul>
<li><a class="reference internal" href="#src.module.Loss.backward"><code class="docutils literal notranslate"><span class="pre">Loss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Loss.forward"><code class="docutils literal notranslate"><span class="pre">Loss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.module.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul>
<li><a class="reference internal" href="#src.module.Module.backward_delta"><code class="docutils literal notranslate"><span class="pre">Module.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Module.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.calculate_gain"><code class="docutils literal notranslate"><span class="pre">Module.calculate_gain()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.update_parameters"><code class="docutils literal notranslate"><span class="pre">Module.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src">Module contents</a><ul>
<li><a class="reference internal" href="#src.AvgPool1D"><code class="docutils literal notranslate"><span class="pre">AvgPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.AvgPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.forward"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.BCELoss"><code class="docutils literal notranslate"><span class="pre">BCELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.BCELoss.backward"><code class="docutils literal notranslate"><span class="pre">BCELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.BCELoss.forward"><code class="docutils literal notranslate"><span class="pre">BCELoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.CELogSoftmax"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.CELogSoftmax.backward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.CELogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Conv1D"><code class="docutils literal notranslate"><span class="pre">Conv1D</span></code></a><ul>
<li><a class="reference internal" href="#src.Conv1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.forward"><code class="docutils literal notranslate"><span class="pre">Conv1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">Conv1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">Conv1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.CrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a><ul>
<li><a class="reference internal" href="#src.CrossEntropyLoss.backward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.CrossEntropyLoss.forward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Flatten"><code class="docutils literal notranslate"><span class="pre">Flatten</span></code></a><ul>
<li><a class="reference internal" href="#src.Flatten.backward_delta"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.forward"><code class="docutils literal notranslate"><span class="pre">Flatten.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.update_parameters"><code class="docutils literal notranslate"><span class="pre">Flatten.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.zero_grad"><code class="docutils literal notranslate"><span class="pre">Flatten.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a><ul>
<li><a class="reference internal" href="#src.Linear.backward_delta"><code class="docutils literal notranslate"><span class="pre">Linear.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Linear.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.forward"><code class="docutils literal notranslate"><span class="pre">Linear.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.update_parameters"><code class="docutils literal notranslate"><span class="pre">Linear.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.zero_grad"><code class="docutils literal notranslate"><span class="pre">Linear.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.LogSoftmax"><code class="docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.LogSoftmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Loss"><code class="docutils literal notranslate"><span class="pre">Loss</span></code></a><ul>
<li><a class="reference internal" href="#src.Loss.backward"><code class="docutils literal notranslate"><span class="pre">Loss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.Loss.forward"><code class="docutils literal notranslate"><span class="pre">Loss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.MSELoss"><code class="docutils literal notranslate"><span class="pre">MSELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.MSELoss.backward"><code class="docutils literal notranslate"><span class="pre">MSELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.MSELoss.forward"><code class="docutils literal notranslate"><span class="pre">MSELoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.MaxPool1D"><code class="docutils literal notranslate"><span class="pre">MaxPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.MaxPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.forward"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul>
<li><a class="reference internal" href="#src.Module.backward_delta"><code class="docutils literal notranslate"><span class="pre">Module.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Module.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.calculate_gain"><code class="docutils literal notranslate"><span class="pre">Module.calculate_gain()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.update_parameters"><code class="docutils literal notranslate"><span class="pre">Module.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Optim"><code class="docutils literal notranslate"><span class="pre">Optim</span></code></a><ul>
<li><a class="reference internal" href="#src.Optim.SGD"><code class="docutils literal notranslate"><span class="pre">Optim.SGD()</span></code></a></li>
<li><a class="reference internal" href="#src.Optim.SGD_eval"><code class="docutils literal notranslate"><span class="pre">Optim.SGD_eval()</span></code></a></li>
<li><a class="reference internal" href="#src.Optim.score"><code class="docutils literal notranslate"><span class="pre">Optim.score()</span></code></a></li>
<li><a class="reference internal" href="#src.Optim.step"><code class="docutils literal notranslate"><span class="pre">Optim.step()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.ReLU"><code class="docutils literal notranslate"><span class="pre">ReLU</span></code></a><ul>
<li><a class="reference internal" href="#src.ReLU.backward_delta"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.forward"><code class="docutils literal notranslate"><span class="pre">ReLU.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.update_parameters"><code class="docutils literal notranslate"><span class="pre">ReLU.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.zero_grad"><code class="docutils literal notranslate"><span class="pre">ReLU.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Sequential"><code class="docutils literal notranslate"><span class="pre">Sequential</span></code></a><ul>
<li><a class="reference internal" href="#src.Sequential.add"><code class="docutils literal notranslate"><span class="pre">Sequential.add()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.backward"><code class="docutils literal notranslate"><span class="pre">Sequential.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.forward"><code class="docutils literal notranslate"><span class="pre">Sequential.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.insert"><code class="docutils literal notranslate"><span class="pre">Sequential.insert()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.reset"><code class="docutils literal notranslate"><span class="pre">Sequential.reset()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sequential.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sequential.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Sigmoid"><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code></a><ul>
<li><a class="reference internal" href="#src.Sigmoid.backward_delta"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.forward"><code class="docutils literal notranslate"><span class="pre">Sigmoid.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sigmoid.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sigmoid.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Softmax"><code class="docutils literal notranslate"><span class="pre">Softmax</span></code></a><ul>
<li><a class="reference internal" href="#src.Softmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.forward"><code class="docutils literal notranslate"><span class="pre">Softmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Softplus"><code class="docutils literal notranslate"><span class="pre">Softplus</span></code></a><ul>
<li><a class="reference internal" href="#src.Softplus.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.forward"><code class="docutils literal notranslate"><span class="pre">Softplus.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softplus.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softplus.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.TanH"><code class="docutils literal notranslate"><span class="pre">TanH</span></code></a><ul>
<li><a class="reference internal" href="#src.TanH.backward_delta"><code class="docutils literal notranslate"><span class="pre">TanH.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">TanH.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.forward"><code class="docutils literal notranslate"><span class="pre">TanH.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.update_parameters"><code class="docutils literal notranslate"><span class="pre">TanH.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.zero_grad"><code class="docutils literal notranslate"><span class="pre">TanH.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/scripts/furo.js"></script>
    <script src="_static/translations.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>