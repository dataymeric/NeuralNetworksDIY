<!DOCTYPE html>
<html class="writer-html5" lang="fr" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>src package &mdash; Documentation Neural Networks DIY </title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/translations.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Recherche" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Neural Networks DIY
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Rechercher docs" aria-label="Rechercher docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">src package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-src.activation">src.activation module</a><ul>
<li><a class="reference internal" href="#src.activation.LeakyReLU"><code class="docutils literal notranslate"><span class="pre">LeakyReLU</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.LeakyReLU.backward_delta"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.forward"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.update_parameters"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LeakyReLU.zero_grad"><code class="docutils literal notranslate"><span class="pre">LeakyReLU.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.LogSoftmax"><code class="docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.LogSoftmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.LogSoftmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.ReLU"><code class="docutils literal notranslate"><span class="pre">ReLU</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.ReLU.backward_delta"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.forward"><code class="docutils literal notranslate"><span class="pre">ReLU.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.update_parameters"><code class="docutils literal notranslate"><span class="pre">ReLU.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.ReLU.zero_grad"><code class="docutils literal notranslate"><span class="pre">ReLU.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.Sigmoid"><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.Sigmoid.backward_delta"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.forward"><code class="docutils literal notranslate"><span class="pre">Sigmoid.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sigmoid.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Sigmoid.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sigmoid.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.Softmax"><code class="docutils literal notranslate"><span class="pre">Softmax</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.Softmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.forward"><code class="docutils literal notranslate"><span class="pre">Softmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.Softplus"><code class="docutils literal notranslate"><span class="pre">Softplus</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.Softplus.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.forward"><code class="docutils literal notranslate"><span class="pre">Softplus.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softplus.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.Softplus.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softplus.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.StableSigmoid"><code class="docutils literal notranslate"><span class="pre">StableSigmoid</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.StableSigmoid.backward_delta"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.forward"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.update_parameters"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.StableSigmoid.zero_grad"><code class="docutils literal notranslate"><span class="pre">StableSigmoid.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.activation.TanH"><code class="docutils literal notranslate"><span class="pre">TanH</span></code></a><ul>
<li><a class="reference internal" href="#src.activation.TanH.backward_delta"><code class="docutils literal notranslate"><span class="pre">TanH.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">TanH.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.forward"><code class="docutils literal notranslate"><span class="pre">TanH.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.update_parameters"><code class="docutils literal notranslate"><span class="pre">TanH.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.activation.TanH.zero_grad"><code class="docutils literal notranslate"><span class="pre">TanH.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.convolution">src.convolution module</a><ul>
<li><a class="reference internal" href="#src.convolution.AvgPool1D"><code class="docutils literal notranslate"><span class="pre">AvgPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.forward"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.AvgPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.convolution.Conv1D"><code class="docutils literal notranslate"><span class="pre">Conv1D</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.Conv1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.forward"><code class="docutils literal notranslate"><span class="pre">Conv1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">Conv1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Conv1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">Conv1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.convolution.Flatten"><code class="docutils literal notranslate"><span class="pre">Flatten</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.Flatten.backward_delta"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.forward"><code class="docutils literal notranslate"><span class="pre">Flatten.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.update_parameters"><code class="docutils literal notranslate"><span class="pre">Flatten.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.Flatten.zero_grad"><code class="docutils literal notranslate"><span class="pre">Flatten.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D"><code class="docutils literal notranslate"><span class="pre">MaxPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.forward"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.convolution.MaxPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.encapsulation">src.encapsulation module</a><ul>
<li><a class="reference internal" href="#src.encapsulation.Optim"><code class="docutils literal notranslate"><span class="pre">Optim</span></code></a><ul>
<li><a class="reference internal" href="#src.encapsulation.Optim.SGD"><code class="docutils literal notranslate"><span class="pre">Optim.SGD()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Optim.SGD_eval"><code class="docutils literal notranslate"><span class="pre">Optim.SGD_eval()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Optim.score"><code class="docutils literal notranslate"><span class="pre">Optim.score()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Optim.step"><code class="docutils literal notranslate"><span class="pre">Optim.step()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.encapsulation.Sequential"><code class="docutils literal notranslate"><span class="pre">Sequential</span></code></a><ul>
<li><a class="reference internal" href="#src.encapsulation.Sequential.add"><code class="docutils literal notranslate"><span class="pre">Sequential.add()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.backward"><code class="docutils literal notranslate"><span class="pre">Sequential.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.forward"><code class="docutils literal notranslate"><span class="pre">Sequential.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.insert"><code class="docutils literal notranslate"><span class="pre">Sequential.insert()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.reset"><code class="docutils literal notranslate"><span class="pre">Sequential.reset()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sequential.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.encapsulation.Sequential.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sequential.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.linear">src.linear module</a><ul>
<li><a class="reference internal" href="#src.linear.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a><ul>
<li><a class="reference internal" href="#src.linear.Linear.backward_delta"><code class="docutils literal notranslate"><span class="pre">Linear.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Linear.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.forward"><code class="docutils literal notranslate"><span class="pre">Linear.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.update_parameters"><code class="docutils literal notranslate"><span class="pre">Linear.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.linear.Linear.zero_grad"><code class="docutils literal notranslate"><span class="pre">Linear.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.loss">src.loss module</a><ul>
<li><a class="reference internal" href="#src.loss.BCELoss"><code class="docutils literal notranslate"><span class="pre">BCELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.BCELoss.backward"><code class="docutils literal notranslate"><span class="pre">BCELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.BCELoss.forward"><code class="docutils literal notranslate"><span class="pre">BCELoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.loss.CELogSoftmax"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.CELogSoftmax.backward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.CELogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.loss.CrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.CrossEntropyLoss.backward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.CrossEntropyLoss.forward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.loss.MSELoss"><code class="docutils literal notranslate"><span class="pre">MSELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.loss.MSELoss.backward"><code class="docutils literal notranslate"><span class="pre">MSELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.loss.MSELoss.forward"><code class="docutils literal notranslate"><span class="pre">MSELoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src.module">src.module module</a><ul>
<li><a class="reference internal" href="#src.module.Loss"><code class="docutils literal notranslate"><span class="pre">Loss</span></code></a><ul>
<li><a class="reference internal" href="#src.module.Loss.backward"><code class="docutils literal notranslate"><span class="pre">Loss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Loss.forward"><code class="docutils literal notranslate"><span class="pre">Loss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.module.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul>
<li><a class="reference internal" href="#src.module.Module.backward_delta"><code class="docutils literal notranslate"><span class="pre">Module.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Module.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.calculate_gain"><code class="docutils literal notranslate"><span class="pre">Module.calculate_gain()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.update_parameters"><code class="docutils literal notranslate"><span class="pre">Module.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.module.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#module-src">Module contents</a><ul>
<li><a class="reference internal" href="#src.AvgPool1D"><code class="docutils literal notranslate"><span class="pre">AvgPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.AvgPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.forward"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.AvgPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">AvgPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.BCELoss"><code class="docutils literal notranslate"><span class="pre">BCELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.BCELoss.backward"><code class="docutils literal notranslate"><span class="pre">BCELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.BCELoss.forward"><code class="docutils literal notranslate"><span class="pre">BCELoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.CELogSoftmax"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.CELogSoftmax.backward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.CELogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">CELogSoftmax.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Conv1D"><code class="docutils literal notranslate"><span class="pre">Conv1D</span></code></a><ul>
<li><a class="reference internal" href="#src.Conv1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Conv1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.forward"><code class="docutils literal notranslate"><span class="pre">Conv1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">Conv1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Conv1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">Conv1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.CrossEntropyLoss"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code></a><ul>
<li><a class="reference internal" href="#src.CrossEntropyLoss.backward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.CrossEntropyLoss.forward"><code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Flatten"><code class="docutils literal notranslate"><span class="pre">Flatten</span></code></a><ul>
<li><a class="reference internal" href="#src.Flatten.backward_delta"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Flatten.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.forward"><code class="docutils literal notranslate"><span class="pre">Flatten.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.update_parameters"><code class="docutils literal notranslate"><span class="pre">Flatten.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Flatten.zero_grad"><code class="docutils literal notranslate"><span class="pre">Flatten.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Linear"><code class="docutils literal notranslate"><span class="pre">Linear</span></code></a><ul>
<li><a class="reference internal" href="#src.Linear.backward_delta"><code class="docutils literal notranslate"><span class="pre">Linear.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Linear.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.forward"><code class="docutils literal notranslate"><span class="pre">Linear.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.update_parameters"><code class="docutils literal notranslate"><span class="pre">Linear.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Linear.zero_grad"><code class="docutils literal notranslate"><span class="pre">Linear.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.LogSoftmax"><code class="docutils literal notranslate"><span class="pre">LogSoftmax</span></code></a><ul>
<li><a class="reference internal" href="#src.LogSoftmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.forward"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.LogSoftmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">LogSoftmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Loss"><code class="docutils literal notranslate"><span class="pre">Loss</span></code></a><ul>
<li><a class="reference internal" href="#src.Loss.backward"><code class="docutils literal notranslate"><span class="pre">Loss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.Loss.forward"><code class="docutils literal notranslate"><span class="pre">Loss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.MSELoss"><code class="docutils literal notranslate"><span class="pre">MSELoss</span></code></a><ul>
<li><a class="reference internal" href="#src.MSELoss.backward"><code class="docutils literal notranslate"><span class="pre">MSELoss.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.MSELoss.forward"><code class="docutils literal notranslate"><span class="pre">MSELoss.forward()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.MaxPool1D"><code class="docutils literal notranslate"><span class="pre">MaxPool1D</span></code></a><ul>
<li><a class="reference internal" href="#src.MaxPool1D.backward_delta"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.forward"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.update_parameters"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.MaxPool1D.zero_grad"><code class="docutils literal notranslate"><span class="pre">MaxPool1D.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul>
<li><a class="reference internal" href="#src.Module.backward_delta"><code class="docutils literal notranslate"><span class="pre">Module.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Module.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.calculate_gain"><code class="docutils literal notranslate"><span class="pre">Module.calculate_gain()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.update_parameters"><code class="docutils literal notranslate"><span class="pre">Module.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Optim"><code class="docutils literal notranslate"><span class="pre">Optim</span></code></a><ul>
<li><a class="reference internal" href="#src.Optim.SGD"><code class="docutils literal notranslate"><span class="pre">Optim.SGD()</span></code></a></li>
<li><a class="reference internal" href="#src.Optim.SGD_eval"><code class="docutils literal notranslate"><span class="pre">Optim.SGD_eval()</span></code></a></li>
<li><a class="reference internal" href="#src.Optim.score"><code class="docutils literal notranslate"><span class="pre">Optim.score()</span></code></a></li>
<li><a class="reference internal" href="#src.Optim.step"><code class="docutils literal notranslate"><span class="pre">Optim.step()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.ReLU"><code class="docutils literal notranslate"><span class="pre">ReLU</span></code></a><ul>
<li><a class="reference internal" href="#src.ReLU.backward_delta"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">ReLU.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.forward"><code class="docutils literal notranslate"><span class="pre">ReLU.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.update_parameters"><code class="docutils literal notranslate"><span class="pre">ReLU.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.ReLU.zero_grad"><code class="docutils literal notranslate"><span class="pre">ReLU.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Sequential"><code class="docutils literal notranslate"><span class="pre">Sequential</span></code></a><ul>
<li><a class="reference internal" href="#src.Sequential.add"><code class="docutils literal notranslate"><span class="pre">Sequential.add()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.backward"><code class="docutils literal notranslate"><span class="pre">Sequential.backward()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.forward"><code class="docutils literal notranslate"><span class="pre">Sequential.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.insert"><code class="docutils literal notranslate"><span class="pre">Sequential.insert()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.reset"><code class="docutils literal notranslate"><span class="pre">Sequential.reset()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sequential.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Sequential.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sequential.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Sigmoid"><code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code></a><ul>
<li><a class="reference internal" href="#src.Sigmoid.backward_delta"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Sigmoid.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.forward"><code class="docutils literal notranslate"><span class="pre">Sigmoid.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.update_parameters"><code class="docutils literal notranslate"><span class="pre">Sigmoid.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Sigmoid.zero_grad"><code class="docutils literal notranslate"><span class="pre">Sigmoid.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Softmax"><code class="docutils literal notranslate"><span class="pre">Softmax</span></code></a><ul>
<li><a class="reference internal" href="#src.Softmax.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softmax.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.forward"><code class="docutils literal notranslate"><span class="pre">Softmax.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softmax.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Softmax.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softmax.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.Softplus"><code class="docutils literal notranslate"><span class="pre">Softplus</span></code></a><ul>
<li><a class="reference internal" href="#src.Softplus.backward_delta"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">Softplus.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.forward"><code class="docutils literal notranslate"><span class="pre">Softplus.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.update_parameters"><code class="docutils literal notranslate"><span class="pre">Softplus.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.Softplus.zero_grad"><code class="docutils literal notranslate"><span class="pre">Softplus.zero_grad()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#src.TanH"><code class="docutils literal notranslate"><span class="pre">TanH</span></code></a><ul>
<li><a class="reference internal" href="#src.TanH.backward_delta"><code class="docutils literal notranslate"><span class="pre">TanH.backward_delta()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.backward_update_gradient"><code class="docutils literal notranslate"><span class="pre">TanH.backward_update_gradient()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.forward"><code class="docutils literal notranslate"><span class="pre">TanH.forward()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.update_parameters"><code class="docutils literal notranslate"><span class="pre">TanH.update_parameters()</span></code></a></li>
<li><a class="reference internal" href="#src.TanH.zero_grad"><code class="docutils literal notranslate"><span class="pre">TanH.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Neural Networks DIY</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">src package</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/src.rst.txt" rel="nofollow"> Afficher la source de la page</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="src-package">
<h1>src package<a class="headerlink" href="#src-package" title="Lien permanent vers cette rubrique"></a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Lien permanent vers cette rubrique"></a></h2>
</section>
<section id="module-src.activation">
<span id="src-activation-module"></span><h2>src.activation module<a class="headerlink" href="#module-src.activation" title="Lien permanent vers cette rubrique"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.activation.LeakyReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">LeakyReLU</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Leaky ReLU activation function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{LeakyReLU}(x) = \max(\alpha x, x) =
\begin{cases}
x, &amp; \text{ if } x \geq 0 \\
\alpha \times x, &amp; \text{ otherwise }
\end{cases}\end{split}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LeakyReLU.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LeakyReLU.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LeakyReLU.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.LogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">LogSoftmax</span></span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>LogSoftmax activation function.</p>
<div class="math notranslate nohighlight">
\[\text{LogSoftmax}(x_{i}) =
\log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)} \right)\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.LogSoftmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.LogSoftmax.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><a class="reference internal" href="_modules/src/activation.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>ReLU (rectified linear unit) activation function.</p>
<div class="math notranslate nohighlight">
\[\text{ReLU}(x) = x^+ = \max(0, x)\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>math:: f”(x) = 1        ext{if} x &gt; 0   ext{else} 0.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.ReLU.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.ReLU.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><a class="reference internal" href="_modules/src/activation.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sigmoid activation function.</p>
<div class="math notranslate nohighlight">
\[\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Sigmoid.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Sigmoid.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.Softmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><a class="reference internal" href="_modules/src/activation.html#Softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Softmax activation function.</p>
<div class="math notranslate nohighlight">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softmax.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.Softplus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">Softplus</span></span><a class="reference internal" href="_modules/src/activation.html#Softplus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Smooth approximation of the ReLU activation function.</p>
<div class="math notranslate nohighlight">
\[\text{Softplus}(x) = \ln(1+e^x)\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.Softplus.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.Softplus.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.StableSigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">StableSigmoid</span></span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Numerically stable Sigmoid activation function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.StableSigmoid.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#StableSigmoid.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.StableSigmoid.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.activation.TanH">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.activation.</span></span><span class="sig-name descname"><span class="pre">TanH</span></span><a class="reference internal" href="_modules/src/activation.html#TanH"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Hyperbolic Tangent activation function.</p>
<div class="math notranslate nohighlight">
\[\text{TanH}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.activation.TanH.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.activation.TanH.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.convolution">
<span id="src-convolution-module"></span><h2>src.convolution module<a class="headerlink" href="#module-src.convolution" title="Lien permanent vers cette rubrique"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">AvgPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D average pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>length</em><em>, </em><em>chan_in</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>(</em><em>length - k_size</em><em>) </em><em>// stride + 1</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.AvgPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.AvgPool1D.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.convolution.Conv1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">Conv1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'xavier_normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>chan_in</strong> (<em>int</em>) – Number of channels in the input image.</p></li>
<li><p><strong>chan_out</strong> (<em>in</em>) – Number of channels produced by the convolution.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;xavier_normal&quot;</em>) – Change the initialization of parameters.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>length</em><em>, </em><em>chan_in</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>(</em><em>length - k_size</em><em>) </em><em>// stride + 1</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
<li><p><strong>Weight</strong> (<em>ndarray</em><em> (</em><em>k_size</em><em>, </em><em>chan_in</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
<li><p><strong>Bias</strong> (<em>ndarray</em><em> (</em><em>chan_out</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Conv1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Conv1D.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.convolution.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><a class="reference internal" href="_modules/src/convolution.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Flatten an output.</p>
<section id="shape">
<h3>Shape<a class="headerlink" href="#shape" title="Lien permanent vers cette rubrique"></a></h3>
<p>Input : ndarray (batch, length, chan_in)
Output : ndarray (batch, length * chan_in)</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.Flatten.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.Flatten.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.convolution.</span></span><span class="sig-name descname"><span class="pre">MaxPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D max pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>length</em><em>, </em><em>chan_in</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>(</em><em>length - k_size</em><em>) </em><em>// stride + 1</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.convolution.MaxPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.convolution.MaxPool1D.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.encapsulation">
<span id="src-encapsulation-module"></span><h2>src.encapsulation module<a class="headerlink" href="#module-src.encapsulation" title="Lien permanent vers cette rubrique"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.encapsulation.Optim">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.encapsulation.</span></span><span class="sig-name descname"><span class="pre">Optim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><span class="pre">Loss</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.SGD">
<span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.SGD" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.SGD_eval">
<span class="sig-name descname"><span class="pre">SGD_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_train</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_test</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dataframe</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fig</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.SGD_eval" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.score" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Optim.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Optim.step" title="Lien permanent vers cette définition"></a></dt>
<dd><p>TODO
y_hat = self.network.forward(batch_x).reshape(-1, 1)  # (batchsize, 1)
Il faut fix ce reshape, il broke en multiclass en reshapant de (batchsize=8, 2 class) =&gt; (16, 1)</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.encapsulation.Sequential">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.encapsulation.</span></span><span class="sig-name descname"><span class="pre">Sequential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.add"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.add" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Add a module to the network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.insert">
<span class="sig-name descname"><span class="pre">insert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.insert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.insert" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Insert a module to the network at a specified indice.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.reset" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Reset network to initial parameters and modules.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.encapsulation.Sequential.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.encapsulation.Sequential.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-src.linear">
<span id="src-linear-module"></span><h2>src.linear module<a class="headerlink" href="#module-src.linear" title="Lien permanent vers cette rubrique"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.linear.Linear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.linear.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Linear module.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – Size of input sample.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Size of output sample.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;normal&quot;</em>) – Change the initialization of parameters.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>input_size</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>output_size</em><em>)</em>) – </p></li>
<li><p><strong>Weight</strong> (<em>ndarray</em><em> (</em><em>input_size</em><em>, </em><em>output_size</em><em>)</em>) – </p></li>
<li><p><strong>Bias</strong> (<em>ndarray</em><em> (</em><em>1</em><em>, </em><em>output_size</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Forward pass.</p>
<p class="rubric">Notes</p>
<p>X &#64; w = (batch, input_size) &#64; (input_size, output_size) = (batch, output_size)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.linear.Linear.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.linear.Linear.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-src.loss">
<span id="src-loss-module"></span><h2>src.loss module<a class="headerlink" href="#module-src.loss" title="Lien permanent vers cette rubrique"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.loss.BCELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">BCELoss</span></span><a class="reference internal" href="_modules/src/loss.html#BCELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.BCELoss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Binary Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.BCELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.BCELoss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.BCELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.BCELoss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.loss.CELogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">CELogSoftmax</span></span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CELogSoftmax" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>TO DO</p>
<div class="math notranslate nohighlight">
\[\text{CE}(y, \hat{y}) = - \log \frac {e^{\hat{y}_y}} {\sum_{i=1}^{K}
e^{\hat{y}_i}} = -\hat{y}_y} + \log \sum_{i=1}^{K}e^{\hat{y}_i}}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CELogSoftmax.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CELogSoftmax.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CELogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CELogSoftmax.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.loss.CrossEntropyLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLoss</span></span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CrossEntropyLoss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CrossEntropyLoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CrossEntropyLoss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.CrossEntropyLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.CrossEntropyLoss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.loss.MSELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.loss.</span></span><span class="sig-name descname"><span class="pre">MSELoss</span></span><a class="reference internal" href="_modules/src/loss.html#MSELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.MSELoss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Mean Squared Error loss function.</p>
<div class="math notranslate nohighlight">
\[MSE = ||y - \hat{y}||^2\]</div>
<div class="math notranslate nohighlight">
\[\nabla_{MSE} = -2(y - \hat{y})\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.loss.MSELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.MSELoss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.loss.MSELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.loss.MSELoss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-src.module">
<span id="src-module-module"></span><h2>src.module module<a class="headerlink" href="#module-src.module" title="Lien permanent vers cette rubrique"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.module.Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.module.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="reference internal" href="_modules/src/module.html#Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Loss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.module.Loss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Loss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Loss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.module.Module">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.module.</span></span><span class="sig-name descname"><span class="pre">Module</span></span><a class="reference internal" href="_modules/src/module.html#Module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.calculate_gain">
<span class="sig-name descname"><span class="pre">calculate_gain</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.calculate_gain"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.calculate_gain" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.module.Module.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.module.Module.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-src">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-src" title="Lien permanent vers cette rubrique"></a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="src.AvgPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">AvgPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D average pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>length</em><em>, </em><em>chan_in</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>(</em><em>length - k_size</em><em>) </em><em>// stride + 1</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.AvgPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#AvgPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.AvgPool1D.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.BCELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">BCELoss</span></span><a class="reference internal" href="_modules/src/loss.html#BCELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.BCELoss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Binary Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.BCELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.BCELoss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.BCELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#BCELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.BCELoss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.CELogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">CELogSoftmax</span></span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CELogSoftmax" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>TO DO</p>
<div class="math notranslate nohighlight">
\[\text{CE}(y, \hat{y}) = - \log \frac {e^{\hat{y}_y}} {\sum_{i=1}^{K}
e^{\hat{y}_i}} = -\hat{y}_y} + \log \sum_{i=1}^{K}e^{\hat{y}_i}}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.CELogSoftmax.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CELogSoftmax.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.CELogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CELogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CELogSoftmax.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Conv1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Conv1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_in</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chan_out</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'xavier_normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D convolution.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>chan_in</strong> (<em>int</em>) – Number of channels in the input image.</p></li>
<li><p><strong>chan_out</strong> (<em>in</em>) – Number of channels produced by the convolution.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;xavier_normal&quot;</em>) – Change the initialization of parameters.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>length</em><em>, </em><em>chan_in</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>(</em><em>length - k_size</em><em>) </em><em>// stride + 1</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
<li><p><strong>Weight</strong> (<em>ndarray</em><em> (</em><em>k_size</em><em>, </em><em>chan_in</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
<li><p><strong>Bias</strong> (<em>ndarray</em><em> (</em><em>chan_out</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Conv1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Conv1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Conv1D.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.CrossEntropyLoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">CrossEntropyLoss</span></span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CrossEntropyLoss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Cross Entropy loss function.</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.CrossEntropyLoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CrossEntropyLoss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.CrossEntropyLoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#CrossEntropyLoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.CrossEntropyLoss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Flatten">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Flatten</span></span><a class="reference internal" href="_modules/src/convolution.html#Flatten"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Flatten an output.</p>
<section id="id1">
<h3>Shape<a class="headerlink" href="#id1" title="Lien permanent vers cette rubrique"></a></h3>
<p>Input : ndarray (batch, length, chan_in)
Output : ndarray (batch, length * chan_in)</p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Flatten.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#Flatten.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Flatten.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Linear">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Linear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'normal'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Linear module.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – Size of input sample.</p></li>
<li><p><strong>output_size</strong> (<em>int</em>) – Size of output sample.</p></li>
<li><p><strong>bias</strong> (<em>bool</em><em>, </em><em>optional</em><em>, </em><em>default=False</em>) – If True, adds a learnable bias to the output.</p></li>
<li><p><strong>init_type</strong> (<em>str</em><em>, </em><em>optional</em><em>, </em><em>default=&quot;normal&quot;</em>) – Change the initialization of parameters.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>input_size</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>output_size</em><em>)</em>) – </p></li>
<li><p><strong>Weight</strong> (<em>ndarray</em><em> (</em><em>input_size</em><em>, </em><em>output_size</em><em>)</em>) – </p></li>
<li><p><strong>Bias</strong> (<em>ndarray</em><em> (</em><em>1</em><em>, </em><em>output_size</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Forward pass.</p>
<p class="rubric">Notes</p>
<p>X &#64; w = (batch, input_size) &#64; (input_size, output_size) = (batch, output_size)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Linear.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/linear.html#Linear.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Linear.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.LogSoftmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">LogSoftmax</span></span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>LogSoftmax activation function.</p>
<div class="math notranslate nohighlight">
\[\text{LogSoftmax}(x_{i}) =
\log \left( \frac{\exp(x_i)}{\sum_j \exp(x_j)} \right)\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.LogSoftmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#LogSoftmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.LogSoftmax.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Loss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Loss</span></span><a class="reference internal" href="_modules/src/module.html#Loss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Loss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Loss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Loss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Loss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Loss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Loss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.MSELoss">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">MSELoss</span></span><a class="reference internal" href="_modules/src/loss.html#MSELoss"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MSELoss" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><code class="xref py py-class docutils literal notranslate"><span class="pre">Loss</span></code></a></p>
<p>Mean Squared Error loss function.</p>
<div class="math notranslate nohighlight">
\[MSE = ||y - \hat{y}||^2\]</div>
<div class="math notranslate nohighlight">
\[\nabla_{MSE} = -2(y - \hat{y})\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.MSELoss.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MSELoss.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MSELoss.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">yhat</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/loss.html#MSELoss.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MSELoss.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.MaxPool1D">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">MaxPool1D</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">k_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>1D max pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">Paramètres<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>k_size</strong> (<em>int</em>) – Size of the convolving kernel.</p></li>
<li><p><strong>stride</strong> (<em>int</em><em>, </em><em>optional</em><em>, </em><em>default=1</em>) – Stride of the convolution.</p></li>
<li><p><strong>Shape</strong> – </p></li>
<li><p><strong>-----</strong> – </p></li>
<li><p><strong>Input</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>length</em><em>, </em><em>chan_in</em><em>)</em>) – </p></li>
<li><p><strong>Output</strong> (<em>ndarray</em><em> (</em><em>batch</em><em>, </em><em>(</em><em>length - k_size</em><em>) </em><em>// stride + 1</em><em>, </em><em>chan_out</em><em>)</em>) – </p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.MaxPool1D.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/convolution.html#MaxPool1D.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.MaxPool1D.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Module">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Module</span></span><a class="reference internal" href="_modules/src/module.html#Module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Module.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.calculate_gain">
<span class="sig-name descname"><span class="pre">calculate_gain</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.calculate_gain"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.calculate_gain" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Module.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/module.html#Module.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Module.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Optim">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Optim</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Loss" title="src.module.Loss"><span class="pre">Loss</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.SGD">
<span class="sig-name descname"><span class="pre">SGD</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.SGD" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.SGD_eval">
<span class="sig-name descname"><span class="pre">SGD_eval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">test_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">network</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.encapsulation.Sequential" title="src.encapsulation.Sequential"><span class="pre">Sequential</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_train</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle_test</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_dataframe</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fig</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.SGD_eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.SGD_eval" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.score"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.score" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Optim.step">
<span class="sig-name descname"><span class="pre">step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_y</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Optim.step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Optim.step" title="Lien permanent vers cette définition"></a></dt>
<dd><p>TODO
y_hat = self.network.forward(batch_x).reshape(-1, 1)  # (batchsize, 1)
Il faut fix ce reshape, il broke en multiclass en reshapant de (batchsize=8, 2 class) =&gt; (16, 1)</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.ReLU">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">ReLU</span></span><a class="reference internal" href="_modules/src/activation.html#ReLU"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>ReLU (rectified linear unit) activation function.</p>
<div class="math notranslate nohighlight">
\[\text{ReLU}(x) = x^+ = \max(0, x)\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>math:: f”(x) = 1        ext{if} x &gt; 0   ext{else} 0.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.ReLU.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#ReLU.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.ReLU.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Sequential">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Sequential</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.add">
<span class="sig-name descname"><span class="pre">add</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.add"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.add" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Add a module to the network.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.backward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.forward" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.insert">
<span class="sig-name descname"><span class="pre">insert</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#src.module.Module" title="src.module.Module"><span class="pre">Module</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.insert"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.insert" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Insert a module to the network at a specified indice.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.reset">
<span class="sig-name descname"><span class="pre">reset</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.reset"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.reset" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Reset network to initial parameters and modules.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sequential.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/encapsulation.html#Sequential.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sequential.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Sigmoid">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Sigmoid</span></span><a class="reference internal" href="_modules/src/activation.html#Sigmoid"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sigmoid activation function.</p>
<div class="math notranslate nohighlight">
\[\text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Sigmoid.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Sigmoid.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Sigmoid.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Softmax">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Softmax</span></span><a class="reference internal" href="_modules/src/activation.html#Softmax"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Softmax activation function.</p>
<div class="math notranslate nohighlight">
\[\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softmax.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softmax.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softmax.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.Softplus">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">Softplus</span></span><a class="reference internal" href="_modules/src/activation.html#Softplus"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Smooth approximation of the ReLU activation function.</p>
<div class="math notranslate nohighlight">
\[\text{Softplus}(x) = \ln(1+e^x)\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.Softplus.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#Softplus.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.Softplus.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="src.TanH">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">src.</span></span><span class="sig-name descname"><span class="pre">TanH</span></span><a class="reference internal" href="_modules/src/activation.html#TanH"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Bases : <a class="reference internal" href="#src.module.Module" title="src.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Hyperbolic Tangent activation function.</p>
<div class="math notranslate nohighlight">
\[\text{TanH}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}\]</div>
<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.backward_delta">
<span class="sig-name descname"><span class="pre">backward_delta</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_delta"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.backward_delta" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Calculates the derivative of the error and the next delta (derivative of the
module with respect to the to the inputs).</p>
<div class="math notranslate nohighlight">
\[\begin{split}\delta_j^{h-1}=\frac{\partial L}{\partial z_j^{h-1}}=\sum_k
\frac{\partial L}{\partial z_k^h} \frac{\partial z_k^h}{\partial z_j^{h-1}},
\text { let } \nabla_{\mathbf{z}^{h-1}} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{z_1^{h-1}} &amp; \frac{\partial z_2^h}{z_1^{h-1}} &amp; \cdots \\
\frac{\partial z_2^h}{z_2^{h-1}} &amp; \ddots &amp; \cdots \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.backward_update_gradient">
<span class="sig-name descname"><span class="pre">backward_update_gradient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.backward_update_gradient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.backward_update_gradient" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update gradient value given module.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial L}{\partial w_i^h}=\sum_k \frac{\partial L}{\partial z_k^h}
\frac{\partial z_k^h}{\partial w_i^h}=\sum_k \delta_k^h
\frac{\partial z_k^h}{\partial w_i^h},
\text { let } \nabla_{\mathbf{w}^h} L=\left(\begin{array}{ccc}
\frac{\partial z_1^h}{\partial w_1^h} &amp; \frac{\partial z_2^h}{\partial w_1^h}
&amp; \cdots \\ \frac{\partial z_1^h}{\partial w_2^h} &amp; \ddots &amp; \\
\vdots &amp; \end{array}\right) \nabla_{\mathbf{z}^h L}\end{split}\]</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.forward" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Passe forward.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.update_parameters">
<span class="sig-name descname"><span class="pre">update_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.update_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.update_parameters" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Update the parameters according to the calculated gradient and the learning
rate.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="src.TanH.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/src/activation.html#TanH.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#src.TanH.zero_grad" title="Lien permanent vers cette définition"></a></dt>
<dd><p>Réinitialise le gradient.</p>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Charles Vin &amp; Aymeric Delefosse.</p>
  </div>

  Compilé avec <a href="https://www.sphinx-doc.org/">Sphinx</a> en utilisant un
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">thème</a>
    fourni par <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>